import io
import os

from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.metrics import AnswerRelevancy
from ragas.dataset_schema import SingleTurnSample

from langchain_openai.embeddings import OpenAIEmbeddings
from langchain import LLMChain
from langchain.chains import RetrievalQA, StuffDocumentsChain
from langchain_openai import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import HumanMessage, SystemMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from unstructured.partition.pdf import partition_pdf

from utils.clean_text import clean_pdf


class FilesService:
    @staticmethod
    def _openai_streamer(retr_qa: RetrievalQA, text: str):
        yield from retr_qa.run(text)

    @staticmethod
    async def query(question, temperature, n_docs, vectorstore):
        llm = ChatOpenAI(
            model_name="gpt-4o", streaming=True, temperature=temperature
        )

        messages = [
            SystemMessage(
                content="Voc√™ √© um algoritmo de classe mundial para responder perguntas com base em documentos."
            ),
            HumanMessage(
                content="Use como base as informa√ß√µes contidas nos documentos para responder √† pergunta:"
            ),
            HumanMessagePromptTemplate.from_template("{context}"),
            HumanMessage(
                content="Dicas: Se voc√™ n√£o encontrar uma resposta relevante nos documentos fornecidos, diga que n√£o sabe e n√£o ir√° poder ajudar."
            ),
            HumanMessage(
                content="Dicas: Se encontrar a resposta relevante nos documentos fornecidos, cite as refer√™ncias e qual documento do banco de dados foi utilizado. Exemplo: 'O primeiro documento cont√©m determinada informa√ß√£o que...'"
            ),
            HumanMessagePromptTemplate.from_template("Pergunta: {question}"),
            HumanMessagePromptTemplate.from_template(
                "Dicas: Se encontrar a resposta com informa√ß√µes relevantes nos documentos similares fornecidos, complemente com um resumo do comportamento ideal para que a solu√ß√£o seja efetiva para a gest√£o p√∫blica para a seguinte quest√£o: {question}"
            ),
             HumanMessagePromptTemplate.from_template(
                "Dicas: separe a resposta um titulo <b>ü™¥ Informa√ß√µes encontradas:</b> quebre em tr√™s linhas e outra parte que √© o resumo do comportamento ideal para a solu√ß√£o em um t√≠tulo <b>üí° Solu√ß√£o efetiva sugerida:</b>"
            ),
            
        ]

        prompt = ChatPromptTemplate(messages=messages)

        qa_chain = LLMChain(llm=llm, prompt=prompt)

        doc_prompt = PromptTemplate(
            template="Conte√∫do do Documento: {page_content}",
            input_variables=["page_content"],
        )

        final_qa_chain = StuffDocumentsChain(
            llm_chain=qa_chain,
            document_variable_name="context",
            document_prompt=doc_prompt,
        )

        retrieval_qa = RetrievalQA(
            retriever=vectorstore.as_retriever(search_kwargs={"k": n_docs}),
            combine_documents_chain=final_qa_chain,
        )

        response = FilesService._openai_streamer(retrieval_qa, question)
        answers = []

        for item in response:
            answer = item.get("answer") if isinstance(item, dict) else str(item)
            if answer:
                answers.append(answer)

        retriever = vectorstore.as_retriever(search_kwargs={"k": n_docs})
        documents = retriever.get_relevant_documents(question)
        retrieved_contexts = [doc.page_content for doc in documents]

        try:
            emb = LangchainEmbeddingsWrapper(
                OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))
            )
            answer_relevancy_metric = AnswerRelevancy(llm=llm, embeddings=emb)
            answers_str = "".join(answers) if isinstance(answers, list) else answers
            retrieved_contexts_str = (
                " ".join(retrieved_contexts)
                if isinstance(retrieved_contexts, list)
                else retrieved_contexts
            )
            row = SingleTurnSample(
                user_input=question,
                response=answers_str,
                retrieved_contexts=[retrieved_contexts_str],
            )
            score = await answer_relevancy_metric.single_turn_ascore(row)
            print("**score**")
            print(score)

        except Exception as e:
            print(f"An error occurred: {e}")

        return answers_str

    @staticmethod
    async def upload(file, chunk_size, vectorstore):
        response = {}

        data = await file.read()
        elements = partition_pdf(file=io.BytesIO(data))
        text = [clean_pdf(ele.text) for ele in elements if ele.text]

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=20,
            length_function=len,
            add_start_index=True,
        )

        docs = text_splitter.create_documents(text)

        for doc in docs:
            doc.metadata = {
                "document_name": file.filename,
                "title": file.filename.split(".")[0],
            }

        vectorstore.add_documents(docs)

        response = {
            "message": "Arquivo carregado com sucesso",
            "filename": file.filename,
            "extracted_text": (
                docs[0].page_content if docs else "Texto n√£o dispon√≠vel"
            ),
            "embedding_size": len(docs) if docs else "Tamanho n√£o dispon√≠vel",
        }
        return response
