{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install langchain-community\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "import unicodedata\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# from unstructured.partition.pdf import partition_pdf\n",
    "# import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN THIS COMMAND JUST ONE TIME LOCALLY ########################################\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "##### RUN THIS COMMAND JUST ONE TIME LOCALLY ########################################\n",
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory2(directory_path):\n",
    "    # List only files (not directories)\n",
    "    return [directory_path + file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    # List only files (not directories)\n",
    "    return [file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "def pdf_to_string(file_name):\n",
    "    loader = PyMuPDFLoader(bronze_path + file_name)\n",
    "    # here we have a class with metadata\n",
    "    data = loader.load()\n",
    "    \n",
    "    elements = partition_pdf(file=io.BytesIO(data))\n",
    "    print(elements)\n",
    "    \n",
    "    def medalion(text):\n",
    "        print(text)\n",
    "        return text\n",
    "    \n",
    "    text = [medalion(ele.text) for ele in elements]\n",
    "    join_text = [\"\\n\".join(text)] \n",
    "    \n",
    "    print(join_text)\n",
    "    \n",
    "    # this way we get all the text of the pdf in just 1 string\n",
    "    text = ''\n",
    "    for i in range(len(data)):\n",
    "        text = text + data[i].page_content\n",
    "\n",
    "def remove_accents_and_special_characters(file0):\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Normalize the text to separate accents from letters\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove accents by discarding non-ASCII characters\n",
    "    text_without_accents = normalized_text.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove any special characters (keeping letters and numbers)\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text_without_accents)\n",
    "\n",
    "    # Convert the content to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver1_path = file0.replace('silver0', 'silver1')\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver1_path, 'w') as file:\n",
    "        file.write(clean_text)\n",
    "\n",
    "def remove_stopwords(file0):\n",
    "\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Get the list of Portuguese stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "    # Split the string into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a string\n",
    "    result = ' '.join(filtered_text)\n",
    "\n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver2_path = file0.replace('silver1', 'silver2')\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver2_path, 'w') as file:\n",
    "        file.write(result)\n",
    "\n",
    "def read_and_convert_to_lowercase(filename):\n",
    "    # Open the file in read mode with UTF-8 encoding\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Convert the content to lowercase\n",
    "    lowercase_content = content.lower()\n",
    "    \n",
    "    return lowercase_content\n",
    "\n",
    "def lemmatize(file0):\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "    # Process the text with the spaCy NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the lemmatized tokens\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver3_path = file0.replace('silver2', 'silver3')\n",
    "    print(silver3_path)\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver3_path, 'w') as file:\n",
    "        file.write(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 0: we only convert from PDF to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 135 of document ../medallion/bronze/plano-acao-climatica-agro.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 83 of document ../medallion/bronze/plano-acao-climatica-itabirito.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 0 of document ../medallion/bronze/plano-acao-climatica-sp-regiao.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 133 of document ../medallion/bronze/plano-acao-climatica-sp-regiao.pdf\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# here we transfer from Bronze to Silver0\n",
    "bronze_path = '../medallion/bronze/'\n",
    "file_list = list_files_in_directory(bronze_path)\n",
    "\n",
    "for file in file_list:\n",
    "    pdf_to_txt(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 1: remove accents and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../medallion/silver0/plano-acao-adaptacao-climatica-nacional.txt\n",
      "../medallion/silver0/plano-acao-climatica-agro.txt\n",
      "../medallion/silver0/plano-acao-climatica-curitiba.txt\n",
      "../medallion/silver0/plano-acao-climatica-federal.txt\n",
      "../medallion/silver0/plano-acao-climatica-itabirito.txt\n",
      "../medallion/silver0/plano-acao-climatica-joao-pessoa.txt\n",
      "../medallion/silver0/plano-acao-climatica-sp-regiao.txt\n",
      "../medallion/silver0/plano-enfrentamento-mudanca-climatica-nacional.txt\n"
     ]
    }
   ],
   "source": [
    "silver0_path = '../medallion/silver0/'\n",
    "file_list = list_files_in_directory2(silver0_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    remove_accents_and_special_characters(file0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 2: stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver1_path = '../medallion/silver1/'\n",
    "file_list = list_files_in_directory2(silver1_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    remove_stopwords(file0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 3: Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../medallion/silver3/plano-acao-adaptacao-climatica-nacional.txt\n",
      "../medallion/silver3/plano-acao-climatica-agro.txt\n",
      "../medallion/silver3/plano-acao-climatica-curitiba.txt\n",
      "../medallion/silver3/plano-acao-climatica-federal.txt\n",
      "../medallion/silver3/plano-acao-climatica-itabirito.txt\n",
      "../medallion/silver3/plano-acao-climatica-joao-pessoa.txt\n",
      "../medallion/silver3/plano-acao-climatica-sp-regiao.txt\n",
      "../medallion/silver3/plano-enfrentamento-mudanca-climatica-nacional.txt\n"
     ]
    }
   ],
   "source": [
    "silver2_path = '../medallion/silver2/'\n",
    "file_list = list_files_in_directory2(silver2_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    lemmatize(file0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
