{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF\n",
    "# !pip install langchain-community\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN THIS COMMAND JUST ONE TIME LOCALLY ########################################\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN THIS COMMAND JUST ONE TIME LOCALLY ########################################\n",
    "# !python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronze to Silver0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory2(directory_path):\n",
    "    # List only files (not directories)\n",
    "    return [directory_path + file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    # List only files (not directories)\n",
    "    return [file for file in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "def pdf_to_txt(file_name):\n",
    "    loader = PyMuPDFLoader(bronze_path + file_name)\n",
    "    # here we have a class with metadata\n",
    "    data = loader.load()\n",
    "\n",
    "    # this way we get all the text of the pdf in just 1 string\n",
    "    text = ''\n",
    "    for i in range(len(data)):\n",
    "        text = text + data[i].page_content\n",
    "\n",
    "    silver_path = f'../medallion/silver0/{file_name[:-4]}.txt'\n",
    "\n",
    "    with open(silver_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "def remove_accents_and_special_characters(file0):\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Normalize the text to separate accents from letters\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove accents by discarding non-ASCII characters\n",
    "    text_without_accents = normalized_text.encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove any special characters (keeping letters and numbers)\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text_without_accents)\n",
    "\n",
    "    # Convert the content to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver1_path = file0.replace('silver0', 'silver1')\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver1_path, 'w') as file:\n",
    "        file.write(clean_text)\n",
    "\n",
    "def convert_to_markdown(pdf_path, markdown_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        markdown_content = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            # You can add formatting here, e.g., convert headings, lists, etc.\n",
    "            # Example: treat lines with all caps as headings\n",
    "            markdown_content += \"\\n\\n\" + text\n",
    "\n",
    "    # Save the Markdown content to a file\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as md_file:\n",
    "        md_file.write(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 135 of document ../medallion/bronze/plano-acao-climatica-agro.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 83 of document ../medallion/bronze/plano-acao-climatica-itabirito.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 0 of document ../medallion/bronze/plano-acao-climatica-sp-regiao.pdf\n",
      "  warnings.warn(\n",
      "c:\\Users\\hvall\\Desktop\\workspace\\bcgxchallenge2024\\app\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:299: UserWarning: Warning: Empty content on page 133 of document ../medallion/bronze/plano-acao-climatica-sp-regiao.pdf\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# here we transfer from Bronze to Silver0\n",
    "bronze_path = '../medallion/bronze/'\n",
    "file_list = list_files_in_directory(bronze_path)\n",
    "\n",
    "for file in file_list:\n",
    "    pdf_to_txt(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver0: pdf to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../medallion/silver0/plano-acao-adaptacao-climatica-nacional.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-agro.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-curitiba.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-federal.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-itabirito.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-joao-pessoa.txt',\n",
       " '../medallion/silver0/plano-acao-climatica-sp-regiao.txt',\n",
       " '../medallion/silver0/plano-enfrentamento-mudanca-climatica-nacional.txt']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver0_path = '../medallion/silver0/'\n",
    "list_files_in_directory2(silver0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_to_lowercase(filename):\n",
    "    # Open the file in read mode with UTF-8 encoding\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Convert the content to lowercase\n",
    "    lowercase_content = content.lower()\n",
    "    \n",
    "    return lowercase_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 1: remove accents and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver0_path = '../medallion/silver0_manual/'\n",
    "file_list = list_files_in_directory2(silver0_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    remove_accents_and_special_characters(file0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 2: stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(file0):\n",
    "\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Get the list of Portuguese stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "    # Split the string into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a string\n",
    "    result = ' '.join(filtered_text)\n",
    "\n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver2_path = file0.replace('silver1', 'silver2')\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver2_path, 'w') as file:\n",
    "        file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver1_path = '../medallion/silver1_manual/'\n",
    "file_list = list_files_in_directory2(silver1_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    remove_stopwords(file0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver 3: Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(file0):\n",
    "    # Open the file and read all the content\n",
    "    with open(file0, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "    # Process the text with the spaCy NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the lemmatized tokens\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Replace 'silver0' with 'silver1'\n",
    "    silver3_path = file0.replace('silver2', 'silver3')\n",
    "    print(silver3_path)\n",
    "\n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(silver3_path, 'w') as file:\n",
    "        file.write(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../medallion/silver3_manual/plano-acao-adaptacao-climatica-nacional.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-agro.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-curitiba.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-federal.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-itabirito.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-joao-pessoa.txt\n",
      "../medallion/silver3_manual/plano-acao-climatica-sp-regiao.txt\n",
      "../medallion/silver3_manual/plano-enfrentamento-mudanca-climatica-nacional.txt\n"
     ]
    }
   ],
   "source": [
    "silver2_path = '../medallion/silver2_manual/'\n",
    "file_list = list_files_in_directory2(silver2_path)\n",
    "\n",
    "for file0 in file_list:\n",
    "    lemmatize(file0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tresnove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
