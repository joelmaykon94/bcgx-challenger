{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c63f6d1-1c26-44ab-baea-90f171ba5564",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYCiS4085fuZUnAiyFRBsZxvIdDC_LsxCQCA&s\" alt=\"Descrição da imagem\">\n",
    "</div>\n",
    "\n",
    "\n",
    "# Introdução\n",
    "\n",
    "Modelo prático para Engenharia de dados entre todas as etapas para criação de uma aplicação de **GenAI**\n",
    "\n",
    "## Estrutura do Treinamento\n",
    "\n",
    "O curso será dividido em quatro partes principais:\n",
    "\n",
    "### Parte 1: Tratamento de Dados, Processamento e Criação de Embeddings\n",
    "\n",
    "Nesta seção, vamos explorar o processo essencial de **preparação de dados** para  GenAI\n",
    "\n",
    "- **Limpeza e normalização dos dados**: Técnicas para garantir que os dados estejam prontos para serem usados em modelos GenAI.\n",
    "- **Processamento de dados não estruturados**: Como lidar com texto, imagens e outros tipos de dados não estruturados que são comuns em aplicações de IA generativa.\n",
    "- **Criação de embeddings**: Entender o que são embeddings e como gerá-los.\n",
    "\n",
    "### Parte 2: Configuração de Banco de Dados Vetorizado e Ingestão de Dados\n",
    "\n",
    " **banco de dados vetorizado** para armazenar os embeddings criados:\n",
    "\n",
    "- **Setup do banco de dados**: Como iniciar um bancos de dados vetorizado\n",
    "- **Ingestão de dados**: Como fazer a ingestão dos embeddings para o banco de dados e organizar as informações.\n",
    "- **Vector search**: Como é realizada as buscas no banco de dados vetorizado.\n",
    "\n",
    "### Parte 3: Criação de Prompts Eficientes para Modelos GenAI\n",
    "\n",
    "Finalmente, vamos focar na **criação de prompts** para maximizar a performance dos modelos de IA generativa. Esta parte é crucial para garantir que os modelos produzam resultados de alta qualidade e contexto relevante. Abordaremos:\n",
    "\n",
    "- **Estruturação de prompts**: Como criar prompts que guiem o modelo corretamente e gerem resultados consistentes.\n",
    "- **Ajustes e refinamentos**: Técnicas para ajustar prompts com base nos resultados obtidos.\n",
    "- **Prompt tuning**: Introdução ao conceito de **ajuste de prompt** como uma técnica para personalizar o comportamento de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5886407-e18d-46cd-95af-13486071c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angulskilucas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c024aa-b4a4-4ddd-8db0-e99f3a891cd2",
   "metadata": {},
   "source": [
    "o que é **RAG**?\n",
    "\n",
    "Nem sempre o modelo utilizado irá trazer a informação a qual foi perguntada, isso por que ela pode não ter sido treinada com o dado necessário para responder perguntas especificas.Essa falta de dados fazem que esses modelos não sejam ideais para `chatbots` ou uso para dar informação em bancos por exemplo.\n",
    "\n",
    "**RAG** (Retrieval-augmented generation) resolve isso de forma simples: forneça contexto adicional ao modelo no prompt\n",
    " Por exemplo, se o modelo não tiver informações sobre \"O que é uma quiche?\" você pode modificar o prompt: \"Uma quiche é uma torta salgada feita com ovos, creme e recheios como queijo ou vegetais. O que é uma quiche?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673ccb2-97af-4f16-82d8-3f85a576a49e",
   "metadata": {},
   "source": [
    "# 1.0 Preprocessing Documents\n",
    "\n",
    "\n",
    "Iremos utilizar um documento pdf como exemplo para criação desses embeddings.\n",
    "\n",
    "O documento pdf nada mais é que um mini livro de receitas rápidas (cuja a marca não pode ser falada em voz alta)\n",
    "zzz\n",
    "Iremos tratar o dado desse documento (note que nesse momento não iremos nos importar com as imagens contidas no pdf) removendo textos desnecessarios e que podem confundir a LLM na hora de buscar esses dados\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExM29qNG9xdmwxOTE2Mnptbm1tNTNweDA1ZW44MmthYmN5NG8yZnRxaiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LwHkZcEhYZYFhDrh8X/giphy.webp\" alt=\"data clean\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43ba6a-42cc-4281-a0b2-d7ffb13b2db4",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb9b86e-022c-47b1-a82f-64d1da587de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader('/Users/angulskilucas/Downloads/102850628 (1).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7302d656-9e01-4c27-aa30-6b1752d4f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5d2e3a-cbcb-43ea-a41f-7ce0afc6bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###     Clean text   ###\n",
    "########################\n",
    "\n",
    "def clean_width_unicode(text: str) -> str:\n",
    "    return text.replace(\"\\u200b\", \"\")\n",
    "\n",
    "def extract_title(text: str, max_title_length=30) -> str:\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    split_text = enumerate(text.split(\"\\n\", 3))\n",
    "    title = []\n",
    "    # Split text into lines and process each\n",
    "    for iteration, word in split_text:\n",
    "        word = word.strip()\n",
    "        if iteration > 2 and word.split()[0].lower() in stop_words:\n",
    "            word = \"\"\n",
    "        if len(word) > max_title_length:  # Skip lines that are too long\n",
    "            word = \"\"\n",
    "        if word:\n",
    "            title.append(word)\n",
    "\n",
    "    return \" \".join(title)\n",
    "\n",
    "\n",
    "def extract_ingredients(text: str) -> str:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    match = re.search(r\"Ingredients\\n(.*?)(?:\\n[A-Z]|$)\", text, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"Ingredients not found\"\n",
    "\n",
    "    ingredients_section = match.group(1).split(\"\\n\")\n",
    "\n",
    "    valid_ingredients = []\n",
    "    for line in ingredients_section:\n",
    "\n",
    "        cleaned_line = line.strip()\n",
    "\n",
    "        if cleaned_line and cleaned_line.split()[0].lower() in stop_words:\n",
    "            break\n",
    "\n",
    "        valid_ingredients.append(cleaned_line)\n",
    "\n",
    "    return \"\\n\".join(valid_ingredients)\n",
    "\n",
    "\n",
    "def extract_cooking_instructions(text: str) -> str:\n",
    "    instructions_list = []\n",
    "    for instruction in text.split(\"Cook\"):\n",
    "        # Check if this part contains the first step '1.'\n",
    "\n",
    "        if \"\\n1.\" in instruction:\n",
    "            # Split the text by lines and gather the instructions\n",
    "            bag = instruction.splitlines()\n",
    "            in_instructions = False\n",
    "            current_step = \"\"\n",
    "\n",
    "            for line in bag:\n",
    "                if re.match(r\"^\\d+\\.\", line.strip()):\n",
    "                    if current_step:\n",
    "                        instructions_list.append(current_step.strip())\n",
    "                    # Start a new step\n",
    "                    current_step = line.strip()\n",
    "                    in_instructions = True\n",
    "                elif in_instructions:\n",
    "                    if any(\n",
    "                        keyword in line\n",
    "                        for keyword in [\"Ingredients\", \"Spinach\", \"Cooking\"]\n",
    "                    ):\n",
    "                        break\n",
    "                    # Otherwise, append the line to the current instruction step\n",
    "                    current_step += \" \" + line.strip()\n",
    "\n",
    "            if current_step:\n",
    "                instructions_list.append(current_step.strip())\n",
    "\n",
    "    instructions = \"\\n\".join(instructions_list)\n",
    "    instructions = remove_unwanted_end(instructions)\n",
    "\n",
    "    return instructions\n",
    "\n",
    "\n",
    "def extract_cost_per_portion(text: str) -> str:\n",
    "    match = re.search(r\"£\\d+\\.\\d+\", text)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"unkown\"\n",
    "\n",
    "\n",
    "def remove_unwanted_end(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes unwanted text at the end of the recipe, like cost, portion size, etc.\n",
    "    \"\"\"\n",
    "    unwanted_pattern = r\"(\\n£\\d+.*?Cost.*?portion.*?recipes.*\\d+\\n?)\"\n",
    "    cleaned_text = re.sub(unwanted_pattern, \"\", text, flags=re.DOTALL)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_document(document_list: list) -> list:\n",
    "    for document in document_list:\n",
    "        page_content = document.page_content\n",
    "        page_content = clean_width_unicode(page_content)\n",
    "        portion_cost = extract_cost_per_portion(page_content)\n",
    "        page_content = remove_unwanted_end(page_content)\n",
    "        title = extract_title(page_content)\n",
    "        ingrediants = extract_ingredients(page_content)\n",
    "        instructions = extract_cooking_instructions(page_content)\n",
    "\n",
    "        new_document = f\"\"\"Title: {title}\n",
    "        Ingrediants:\n",
    "        {ingrediants}\n",
    "    \n",
    "        Instructions:\n",
    "        \n",
    "        {instructions}\n",
    "    \n",
    "        Cost per portion: {portion_cost}\n",
    "        \"\"\"\n",
    "        document.page_content = new_document\n",
    "    return document_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b35a65-593b-41ad-a23f-09cf14c0899d",
   "metadata": {},
   "source": [
    "O Conteúdo que nos interessa começa a partir da 5 pagina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d7795a-6fa8-4582-beeb-bac5743795db",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_data = data[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089b3ac3-c0a5-40ff-9f43-b0e39d97499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_data = clean_document(curated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab7875-04ad-4555-baeb-c065b73aed6b",
   "metadata": {},
   "source": [
    "# 1.2 Embeddings\n",
    "\n",
    "Após a limpeza dos dados, a criação dos embeddings é relativamente simples.\n",
    "Com o uso de uma API e um LLM de sua escolha (Gemini, OpenAI, Claude), podemos enviar o documento de texto para ela e em retorno receberemos um vetor `embedding` desses dados.\n",
    "\n",
    "Note que um dos passos\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWg5cjN2b2hneHJtY2g1c2h5ZXB1ODN2d2VmYTd0dWpxejI3cWRzcSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/0lGd2OXXHe4tFhb7Wh/giphy.webp\" alt=\"data clean\"width=\"30%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6a8728-cf69-4271-b23d-063b6d556c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/angulskilucas/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from tqdm import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfee25b8-1626-4d0b-bba9-257bc1500de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "\n",
    "def create_embbedings(text:str)->list:\n",
    "    embedding_chunk_list = []\n",
    "    text_split = text_splitter.split_text(text)\n",
    "\n",
    "    for t in text_split:\n",
    "        emb = embedding.embed_query(t)\n",
    "        embedding_chunk_list.append(emb)\n",
    "\n",
    "    return embedding_chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe7ef14-fa95-4819-9ccf-7fc30552c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY ='AIzaSyBJgt_rRnw87jD39ahu4VltH3RFLuZ_8rs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31056994-2450-4749-8b27-597b4f4a280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814.125\n"
     ]
    }
   ],
   "source": [
    "#Média do tamanho dos documentos\n",
    "avg_doc_str_size = sum([len(curated_data[i].page_content) for i in range(len(curated_data))])/len(curated_data)\n",
    "print(avg_doc_str_size)\n",
    "#Chunk overlap deve ser entre 10% a 25% do tamanho do documento dependendo do documento processado.\n",
    "# Nesse caso o documento é pequeno, não havendo necessidade de reajuste dos chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d4309a-fb16-4dc1-a6a2-dba028b8d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
    "                                         google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba0c1ca-aba7-4d34-9418-4e80501e0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 16/16 [00:35<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings_list = []\n",
    "for index in tqdm(range(len(curated_data))):\n",
    "    try:\n",
    "        emb = create_embbedings(curated_data[index].page_content)\n",
    "        embeddings_list.append(emb)\n",
    "    except:\n",
    "        print(f\"Error while retriving embeddings for document number {index}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c2206-1a40-4843-a902-7766cffc300f",
   "metadata": {},
   "source": [
    "# 2.0 Configuração do banco de dados\n",
    "\n",
    "Existem enúmeras opções para banco de dados vetorizados. nesse momento utizaremos um banco relacional `Postgres` com a extensão `PgVector`\n",
    "\n",
    " Essa extensão oferece diferentes funcionalidades que ajudam a identificar \"nearest neighbors\" de maneira exata ou aproximada. O Pgvector  funciona perfeitamente com outros recursos do PostgreSQL, como index e queries.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://api.pgxn.org/src/postgresql_anonymizer/postgresql_anonymizer-0.2.1/postgresql_anonymizer.logo.gif\" alt=\"data clean\"width=\"30%\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161fe57-3db5-4edc-8d15-f0a3a9e9a628",
   "metadata": {},
   "source": [
    "## 2.1 Criação do banco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb620a-6a03-4604-b801-9f0944a5abe1",
   "metadata": {},
   "source": [
    "Iremos utilizar um `Dockerfile`para criação de nosso banco, já fazendo a instalação da sua extensão.\n",
    "\n",
    "```bash\n",
    "FROM postgres:15\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y postgresql-server-dev-all git build-essential && \\\n",
    "    git clone --branch v0.7.4 https://github.com/pgvector/pgvector.git && \\\n",
    "    cd pgvector && make && make install\n",
    "\n",
    "COPY init.sql /docker-entrypoint-initdb.d/\n",
    "```\n",
    "\n",
    "note que estamos copiando o arquivo `init.sql` ele será responsável por iniciar a extensão\n",
    "\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bdc71-623a-4c26-aaab-6dc713c1eccf",
   "metadata": {},
   "source": [
    "## 2.2 Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e2634e-f46c-4394-bdf3-9b72b759f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c10e6b-fb73-495d-b28d-b2f25692bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql://postgres:postgres@localhost:5432/postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176a5f1c-4ab9-4003-9854-49bd66f4038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(connection_string)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a406326-baf9-4923-bbfd-f5414ae8e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eb47a3a-42a8-4f94-86e0-4d3101ded334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a tabela de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1934be54-55db-4653-85e1-f5cd96eeb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_create_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "    embeddings (\n",
    "    id bigserial primary key, \n",
    "    content text,\n",
    "    metadata json,\n",
    "    embedding vector(768) --Verificar tamanho do vetor\n",
    "    )\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(table_create_command)\n",
    "cur.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acf710-8898-4d90-bed6-17dbfc51384e",
   "metadata": {},
   "source": [
    "## 2.3 Ingestão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3da31199-5e12-402d-b4cd-a9d3153e6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd3727fb-6b60-465b-875b-c800f841f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_data = []\n",
    "for document, embedded_doc in zip(curated_data, embeddings_list):\n",
    "    #print(np.array(embedded_doc[0]).shape)\n",
    "    doc_tuple = (\n",
    "        document.page_content,\n",
    "        json.dumps(document.metadata),\n",
    "        np.array(embedded_doc[0])\n",
    "    )\n",
    "    ingestion_data.append(doc_tuple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77504694-4c8c-41bc-8688-db6bff35c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "execute_values(cur, \"INSERT INTO embeddings (content, metadata, embedding) VALUES %s\", ingestion_data)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48dcfa14-abe2-433d-b722-141f54ab8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de docs 32 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT COUNT(*) as cnt FROM embeddings;\")\n",
    "num_records = cur.fetchone()[0]\n",
    "print(\"qtd de docs\", num_records,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9cc9a-a173-48b8-be12-52b36168147a",
   "metadata": {},
   "source": [
    "## 2.4 Criaçao de index \n",
    "\n",
    "As vezes podemos nos deparar com uma grande quantidade de documentos, e para isso otimizar a busca é algo necessário.\n",
    "\n",
    "O índice IVFFlat funciona dividindo os vetores na tabela em várias listas. O algoritmo calcula um número de centróides e encontra os clusters em torno desses centróides. Assim, existe uma lista para cada centróide, e os elementos dessas listas são os vetores que compõem o cluster correspondente.\n",
    "\n",
    "Então quando executamos a busca, ao invés de calcular a distância para todos os vetores, o espaço de busca é enxugado para apenas para o subconjunto de uma lista.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://tembo.io/_astro/ivfflat.cTzzAfeL_1UaBoa.webp\" alt=\"data clean\"width=\"60%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff7bf0ec-eed4-4511-88b9-491ba7034ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_lists = num_records / 1000\n",
    "if num_lists < 10:\n",
    "   num_lists = 10\n",
    "if num_records > 1000000:\n",
    "   num_lists = math.sqrt(num_records)\n",
    "cur.execute(f'CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {num_lists});')\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64580f9-bc81-4905-bb40-ee12c0386e1c",
   "metadata": {},
   "source": [
    "## 2.5 Vector Search\n",
    "\n",
    "Por default, o PgVector utiliza a similaridade de cosseno para fazer a busca dos vetores, essa operação é descrita através do uso dos `<=>` na query.\n",
    "\n",
    "A Similaridade Cosseno costuma ser mais eficiente que a Distância Euclidiana, pois lida melhor com vetores de diferentes comprimentos.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*LfW66-WsYkFqWc4XYJbEJg.png\" alt=\"data clean\"width=\"30%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5d1264a-888b-4f8f-aeeb-d9238da85904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    response = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        task_type=\"retrieval_query\",\n",
    "        google_api_key=GEMINI_API_KEY,\n",
    "    ).embed_query(text.replace(\"\\n\", \" \"))\n",
    "    return response\n",
    "\n",
    "def retrive_similar_docs(query_embedding, conn, limit=3):\n",
    "    embedding_array = np.array(query_embedding)\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        f\"SELECT content FROM embeddings ORDER BY embedding <=> %s LIMIT {limit}\",\n",
    "        (embedding_array,),\n",
    "    )\n",
    "    top_docs = cur.fetchall()\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fbc3b63-fd71-4ed7-9235-2dba89e585d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = get_embeddings('mushroom recipe')\n",
    "recipes = retrive_similar_docs(input_embedding,conn,limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e7dc441-86a5-476d-9bdf-23d65377fcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Title: Mushroom and Spinach Stroganoff\\n        Ingrediants:\\n        1 Knorr Vegetable Stock Cube\\n600 g mushrooms sliced\\n200 g spinach\\n1 red pepper\\n1 red onion chopped\\n175 ml reduced-fat soured cream\\n200 ml water\\n2 tsp lemon juice\\n1 garlic chopped\\n1 tbsp paprika\\nparsley chopped\\n2 tbsp olive oil\\n300 g basmati and wild rice mix\\n    \\n        Instructions:\\n        \\n        1.\\t Heat the oil in a large frying pan, add the onion then cook on a medium heat for 5 minutes.\\n2.\\t Add the mushrooms, pepper, paprika and garlic then fry gently for 3-4 minutes until slightly browned. Then pour in the water and add the Knorr Vegetable Stock Cube.\\n3.\\t Bring to the boil for 2 minutes then reduce the heat and stir in the soured cream and spinach.\\n4.\\t Simmer for a minute or so, until thickened, then stir in the lemon juice and add the chopped parsley before serving with cooked rice or pappardelle pasta.\\n    \\n        Cost per portion: £1.16\\n        ',),\n",
       " ('Title: Mushroom and Spinach Stroganoff\\n        Ingrediants:\\n        1 Knorr Vegetable Stock Cube\\n600 g mushrooms sliced\\n200 g spinach\\n1 red pepper\\n1 red onion chopped\\n175 ml reduced-fat soured cream\\n200 ml water\\n2 tsp lemon juice\\n1 garlic chopped\\n1 tbsp paprika\\nparsley chopped\\n2 tbsp olive oil\\n300 g basmati and wild rice mix\\n    \\n        Instructions:\\n        \\n        1.\\t Heat the oil in a large frying pan, add the onion then cook on a medium heat for 5 minutes.\\n2.\\t Add the mushrooms, pepper, paprika and garlic then fry gently for 3-4 minutes until slightly browned. Then pour in the water and add the Knorr Vegetable Stock Cube.\\n3.\\t Bring to the boil for 2 minutes then reduce the heat and stir in the soured cream and spinach.\\n4.\\t Simmer for a minute or so, until thickened, then stir in the lemon juice and add the chopped parsley before serving with cooked rice or pappardelle pasta.\\n    \\n        Cost per portion: £1.16\\n        ',),\n",
       " ('Title: Fajita Pockets\\n        Ingrediants:\\n        1 Knorr Veggie Cook’s Essentials Smoked\\n    \\n        Instructions:\\n        \\n        1.\\t Heat the olive oil in a large pan over a medium-high heat and sauté the mushrooms, peppers and onion for 5 minutes until starting to soften.\\n2.\\t Pour over the water and add in Knorr’s Smoked Chilli & Tomato Stock Pot. Cover and gently simmer for a further 10 minutes until the vegetables are soft and the stock has melted into a rich sauce.\\n3.\\t Lay out a tortilla wrap and spoon over the cooked vegetable filling. Top with grated cheddar, diced avocado, diced tomatoes, a few torn coriander leaves and a dollop of soured cream.\\n4.\\t Working clockwise, carefully fold your tortilla around the filling in 6 folds. You should have a neat parcel. Repeat with the rest of your ingredients.\\n5.\\t Fry the pockets in a hot, dry pan for 2-3 minutes on each side. Allow to cool a little, then serve.\\n    \\n        Cost per portion: £1.76\\n        ',)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c6e0f-3645-4558-b21e-b7c4f7bfc328",
   "metadata": {},
   "source": [
    "# 3  Prompts e resgatando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "406bed28-5d36-43a0-9bef-f1b3a4daf601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da4f1db-ad76-487b-809d-adc92a2664dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, temperature=0.6, max_tokens=1000):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=GEMINI_API_KEY,\n",
    "        temperature=temperature,\n",
    "        top_p=0.7,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85f316-67e7-4f98-af01-f556e01d06ca",
   "metadata": {},
   "source": [
    "Tentaremos brincar com a LLM e ela se passar pelo Gordon Ramsay para ensinar aos nosso queridos usuários como se cozinha de verdade\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExNnRycjVuc2k2enQ3dThsZmd3anlmemtid2ZwdzJ2YWYwNWoyOWt0eCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/w8g5zUCbH215kUjycc/giphy.webp\" alt=\"data clean\"width=\"30%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6c9d186-661b-4bc1-b2ae-c7a3c825e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input_with_retrieval(user_input):\n",
    "    delimiter = \"```\"\n",
    "\n",
    "    # Retrieve similar documents based on user input\n",
    "    related_docs = retrive_similar_docs(get_embeddings(user_input), conn,limit=1)\n",
    "    sleep(0.2)\n",
    "    # Ensure there are at least 3 documents\n",
    "    if len(related_docs) ==0:\n",
    "        return \"Not enough relevant documents found.\"\n",
    "\n",
    "    system_message = f\"\"\"\n",
    "        You are a chatbot with the personality and traits of Gordon Ramsay.\\\n",
    "        Users may ask you for quick recipes to cook, and you must answer based on the documents that best fit the user's query.\\\n",
    "        Your instructions must follow the order:1. Ingrediants, 2. Cooking Steps, 3. Cost per portion.\n",
    "        Do not make up ingredients or cooking steps that are not in the documents, and be sure to include the cost per portion.\n",
    "        Recipes: {related_docs[0]}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "    (\n",
    "        \"system\",system_message,\n",
    "    ),\n",
    "    (\"human\", user_input),\n",
    "]\n",
    "\n",
    "    final_response = get_completion_from_messages(messages)\n",
    "    \n",
    "    # Handle empty responses\n",
    "    if not final_response.content:\n",
    "        return \"Sorry, I couldn't generate a response. Please try again.\"\n",
    "\n",
    "    return final_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd3484e0-9c7b-4d9e-9c4d-06e44cab2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = process_input_with_retrieval('how do I make a Ratatouille?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "222fb7fb-054e-43ab-ba46-9b358af03ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen, you want to make a proper Ratatouille, you need to follow these steps precisely. Don't get cocky, this isn't some microwave meal. \n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* 10 grams Knorr Aromat Seasoning\n",
      "* 100 ml olive oil\n",
      "* 2 aubergines finely diced\n",
      "* 2 courgettes finely diced\n",
      "* 2 green peppers finely diced\n",
      "* 2 red peppers finely diced\n",
      "* 4 cloves garlic, chopped\n",
      "* handful of basil leaves, torn\n",
      "* sprig of thyme\n",
      "* 200g Passata\n",
      "\n",
      "**Cooking Steps:**\n",
      "\n",
      "1. Pour the olive oil into a large, deep frying pan and heat until really hot.\n",
      "2. Add in the aubergine and courgette and sprinkle with Knorr Aromat seasoning. Fry for around 4–5 minutes stirring occasionally until lightly browned.\n",
      "3. Remove the vegetables with a slotted spoon. In the same pan, add the diced peppers and garlic season with Aromat and fry until lightly browned, stirring occasionally.\n",
      "4. Pour the tomato sauce into the frying pan, add the peppers and aubergines and cook for 5 minutes to heat through. Add the basil and thyme reserving a little basil for garnish.\n",
      "5. Transfer to a serving dish, garnish with the reserved basil and serve either hot, warm or at room temperature.\n",
      "\n",
      "**Cost per portion:** £1.40\n",
      "\n",
      "Now get to it, don't be a donkey! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70f83e-42f5-4138-919d-0eb0f39776d1",
   "metadata": {},
   "source": [
    "For any queries\n",
    "\n",
    "Angulski.lucas@bcg.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfe51f-b819-486e-a31b-c843ac39db2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
